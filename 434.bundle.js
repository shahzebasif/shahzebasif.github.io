"use strict";(self.webpackChunkshahzebasif=self.webpackChunkshahzebasif||[]).push([[434],{2434:(e,t,a)=>{a.r(t),a.d(t,{default:()=>s});const s='<nav id="TOC"> <ul> <li><a href="#references" id="toc-references"><span class="toc-section-number">1</span> References</a> <ul> <li><a href="#the-algorithm-design-manual" id="toc-the-algorithm-design-manual"><span class="toc-section-number">1.1</span> The Algorithm Design Manual</a></li> <li><a href="#others" id="toc-others"><span class="toc-section-number">1.2</span> Others</a></li> </ul></li> <li><a href="#introduction" id="toc-introduction"><span class="toc-section-number">2</span> Introduction</a> <ul> <li><a href="#what-is-an-algorithm" id="toc-what-is-an-algorithm"><span class="toc-section-number">2.1</span> What is an Algorithm?</a></li> <li><a href="#communicating-algorithms" id="toc-communicating-algorithms"><span class="toc-section-number">2.2</span> Communicating Algorithms</a></li> <li><a href="#correctness" id="toc-correctness"><span class="toc-section-number">2.3</span> Correctness</a> <ul> <li><a href="#incorrectness" id="toc-incorrectness"><span class="toc-section-number">2.3.1</span> Incorrectness</a></li> <li><a href="#induction" id="toc-induction"><span class="toc-section-number">2.3.2</span> Induction</a></li> <li><a href="#summation" id="toc-summation"><span class="toc-section-number">2.3.3</span> Summation</a></li> </ul></li> <li><a href="#modeling" id="toc-modeling"><span class="toc-section-number">2.4</span> Modeling</a></li> </ul></li> <li><a href="#analysis" id="toc-analysis"><span class="toc-section-number">3</span> Analysis</a> <ul> <li><a href="#complexity" id="toc-complexity"><span class="toc-section-number">3.1</span> Complexity</a> <ul> <li><a href="#addition" id="toc-addition"><span class="toc-section-number">3.1.1</span> Addition</a></li> <li><a href="#multiplying" id="toc-multiplying"><span class="toc-section-number">3.1.2</span> Multiplying</a></li> </ul></li> <li><a href="#logarithms" id="toc-logarithms"><span class="toc-section-number">3.2</span> Logarithms</a></li> </ul></li> <li><a href="#data-structures" id="toc-data-structures"><span class="toc-section-number">4</span> Data Structures</a> <ul> <li><a href="#arrays" id="toc-arrays"><span class="toc-section-number">4.1</span> Arrays</a></li> <li><a href="#lists" id="toc-lists"><span class="toc-section-number">4.2</span> Lists</a></li> <li><a href="#stacks" id="toc-stacks"><span class="toc-section-number">4.3</span> Stacks</a></li> <li><a href="#queues" id="toc-queues"><span class="toc-section-number">4.4</span> Queues</a></li> <li><a href="#dictionaries" id="toc-dictionaries"><span class="toc-section-number">4.5</span> Dictionaries</a> <ul> <li><a href="#criteria" id="toc-criteria"><span class="toc-section-number">4.5.1</span> Criteria</a></li> <li><a href="#array" id="toc-array"><span class="toc-section-number">4.5.2</span> Array</a></li> <li><a href="#lists-1" id="toc-lists-1"><span class="toc-section-number">4.5.3</span> Lists</a></li> </ul></li> <li><a href="#binary-search-trees" id="toc-binary-search-trees"><span class="toc-section-number">4.6</span> Binary Search Trees</a> <ul> <li><a href="#traversal" id="toc-traversal"><span class="toc-section-number">4.6.1</span> Traversal</a></li> </ul></li> <li><a href="#priority-queues" id="toc-priority-queues"><span class="toc-section-number">4.7</span> Priority Queues</a></li> <li><a href="#hash-functions" id="toc-hash-functions"><span class="toc-section-number">4.8</span> Hash Functions</a> <ul> <li><a href="#hash-tables" id="toc-hash-tables"><span class="toc-section-number">4.8.1</span> Hash Tables</a></li> <li><a href="#other-uses" id="toc-other-uses"><span class="toc-section-number">4.8.2</span> Other Uses</a></li> </ul></li> </ul></li> <li><a href="#sorting" id="toc-sorting"><span class="toc-section-number">5</span> Sorting</a> <ul> <li><a href="#selection-sort" id="toc-selection-sort"><span class="toc-section-number">5.1</span> Selection Sort</a></li> <li><a href="#insertion-sort" id="toc-insertion-sort"><span class="toc-section-number">5.2</span> Insertion Sort</a></li> <li><a href="#heapsort" id="toc-heapsort"><span class="toc-section-number">5.3</span> Heapsort</a> <ul> <li><a href="#heaps" id="toc-heaps"><span class="toc-section-number">5.3.1</span> Heaps</a></li> <li><a href="#sorting-1" id="toc-sorting-1"><span class="toc-section-number">5.3.2</span> Sorting</a></li> </ul></li> <li><a href="#mergesort" id="toc-mergesort"><span class="toc-section-number">5.4</span> Mergesort</a> <ul> <li><a href="#divide-and-conquer" id="toc-divide-and-conquer"><span class="toc-section-number">5.4.1</span> Divide and Conquer</a></li> <li><a href="#sorting-with-divide-and-conquer" id="toc-sorting-with-divide-and-conquer"><span class="toc-section-number">5.4.2</span> Sorting with Divide-and-Conquer</a></li> <li><a href="#visualizing-mergesort" id="toc-visualizing-mergesort"><span class="toc-section-number">5.4.3</span> Visualizing Mergesort</a></li> <li><a href="#merging-arrays" id="toc-merging-arrays"><span class="toc-section-number">5.4.4</span> Merging Arrays</a></li> <li><a href="#complexity-1" id="toc-complexity-1"><span class="toc-section-number">5.4.5</span> Complexity</a></li> </ul></li> <li><a href="#quicksort" id="toc-quicksort"><span class="toc-section-number">5.5</span> Quicksort</a> <ul> <li><a href="#partitioning" id="toc-partitioning"><span class="toc-section-number">5.5.1</span> Partitioning</a></li> <li><a href="#randomization" id="toc-randomization"><span class="toc-section-number">5.5.2</span> Randomization</a></li> </ul></li> <li><a href="#bucketsort" id="toc-bucketsort"><span class="toc-section-number">5.6</span> Bucketsort</a></li> <li><a href="#intricacies" id="toc-intricacies"><span class="toc-section-number">5.7</span> Intricacies</a> <ul> <li><a href="#choices" id="toc-choices"><span class="toc-section-number">5.7.1</span> Choices</a></li> <li><a href="#special-cases" id="toc-special-cases"><span class="toc-section-number">5.7.2</span> Special Cases</a></li> </ul></li> </ul></li> <li><a href="#graphs" id="toc-graphs"><span class="toc-section-number">6</span> Graphs</a> <ul> <li><a href="#types-and-terminology" id="toc-types-and-terminology"><span class="toc-section-number">6.1</span> Types and Terminology</a></li> <li><a href="#searching-a-graph" id="toc-searching-a-graph"><span class="toc-section-number">6.2</span> Searching a Graph</a> <ul> <li><a href="#breadth-first-search" id="toc-breadth-first-search"><span class="toc-section-number">6.2.1</span> Breadth-First Search</a></li> <li><a href="#depth-first-search" id="toc-depth-first-search"><span class="toc-section-number">6.2.2</span> Depth-First Search</a></li> <li><a href="#implementation" id="toc-implementation"><span class="toc-section-number">6.2.3</span> Implementation</a></li> <li><a href="#more-terminology" id="toc-more-terminology"><span class="toc-section-number">6.2.4</span> More Terminology</a></li> </ul></li> <li><a href="#paths-and-walks" id="toc-paths-and-walks"><span class="toc-section-number">6.3</span> Paths and Walks</a></li> <li><a href="#minimum-spanning-trees" id="toc-minimum-spanning-trees"><span class="toc-section-number">6.4</span> Minimum Spanning Trees</a> <ul> <li><a href="#prims-algorithm" id="toc-prims-algorithm"><span class="toc-section-number">6.4.1</span> Prim’s Algorithm</a></li> <li><a href="#kruskals-algorithm" id="toc-kruskals-algorithm"><span class="toc-section-number">6.4.2</span> Kruskal’s Algorithm</a></li> </ul></li> <li><a href="#shortest-path" id="toc-shortest-path"><span class="toc-section-number">6.5</span> Shortest Path</a> <ul> <li><a href="#dijkstras-algorithm" id="toc-dijkstras-algorithm"><span class="toc-section-number">6.5.1</span> Dijkstra’s Algorithm</a></li> </ul></li> <li><a href="#toplogical-sort" id="toc-toplogical-sort"><span class="toc-section-number">6.6</span> Toplogical Sort</a></li> </ul></li> <li><a href="#searching" id="toc-searching"><span class="toc-section-number">7</span> Searching</a> <ul> <li><a href="#binary-search" id="toc-binary-search"><span class="toc-section-number">7.1</span> Binary Search</a></li> <li><a href="#linear-search" id="toc-linear-search"><span class="toc-section-number">7.2</span> Linear Search</a></li> <li><a href="#backtracking" id="toc-backtracking"><span class="toc-section-number">7.3</span> Backtracking</a> <ul> <li><a href="#visualizing" id="toc-visualizing"><span class="toc-section-number">7.3.1</span> Visualizing</a></li> <li><a href="#sudoku" id="toc-sudoku"><span class="toc-section-number">7.3.2</span> Sudoku</a></li> </ul></li> <li><a href="#simulated-annealing" id="toc-simulated-annealing"><span class="toc-section-number">7.4</span> Simulated Annealing</a></li> <li><a href="#random-sampling" id="toc-random-sampling"><span class="toc-section-number">7.5</span> Random Sampling</a></li> </ul></li> <li><a href="#dynamic-programming" id="toc-dynamic-programming"><span class="toc-section-number">8</span> Dynamic Programming</a> <ul> <li><a href="#memoization" id="toc-memoization"><span class="toc-section-number">8.1</span> Memoization</a></li> <li><a href="#longest-common-subsequence" id="toc-longest-common-subsequence"><span class="toc-section-number">8.2</span> Longest Common Subsequence</a> <ul> <li><a href="#example" id="toc-example"><span class="toc-section-number">8.2.1</span> Example</a></li> <li><a href="#strings" id="toc-strings"><span class="toc-section-number">8.2.2</span> Strings</a></li> <li><a href="#longest-increasing-subsequence" id="toc-longest-increasing-subsequence"><span class="toc-section-number">8.2.3</span> Longest Increasing Subsequence</a></li> </ul></li> </ul></li> </ul> </nav> <h1 data-number="1" id="references"><span class="header-section-number">1</span> References</h1> <h2 data-number="1.1" id="the-algorithm-design-manual"><span class="header-section-number">1.1</span> The Algorithm Design Manual</h2> <p>Most of the following notes are based on The Algorithm Design Manual by S.S.Skiena.</p> <h2 data-number="1.2" id="others"><span class="header-section-number">1.2</span> Others</h2> <ul> <li><p><a href="http://www.cis.upenn.edu/~matuszek/cit594-2012/Pages/backtracking.html">Backtracking</a></p></li> <li><p><a href="http://www.geeksforgeeks.org/dynamic-programming-set-4-longest-common-subsequence/">LCS</a></p></li> <li><p><a href="http://stackoverflow.com/questions/2631726/how-to-determine-the-longest-increasing-subsequence-using-dynamic-programming">LIS</a></p></li> </ul> <h1 data-number="2" id="introduction"><span class="header-section-number">2</span> Introduction</h1> <h2 data-number="2.1" id="what-is-an-algorithm"><span class="header-section-number">2.1</span> What is an Algorithm?</h2> <p>An <span><strong>algorithm</strong></span> is a sequence of steps designed to solve a problem.</p> <p>Each <span><strong>problem</strong></span> has its own <span><strong>instances</strong></span> which are specific and apply to a specific situation.</p> <h2 data-number="2.2" id="communicating-algorithms"><span class="header-section-number">2.2</span> Communicating Algorithms</h2> <p>Algorithms can be expressed in many different ways.</p> <p><span><strong>Pseudocode</strong></span> balances the detail and verbosity of code and the ease of understanding of plain English to represent an algorithm.</p> <p>All algorithms need a clearly defined problem. A badly defined problem will likely give a badly designed algorithm.</p> <h2 data-number="2.3" id="correctness"><span class="header-section-number">2.3</span> Correctness</h2> <p>A large part of designing and using algorithms is verifying that they are actually correct.</p> <p>Formal <span><strong>proofs</strong></span> can get very complicated and are difficult to understand without a math background.</p> <p>But there are other ways of showing that an algorithm is correct besides a rigorous mathematical proof. Note that even obvious "algorithms" can be very incorrect.</p> <p><span><strong>Heuristics</strong></span> are similar to algorithms, in that they attempt to solve problems, but they do not guarantee an optimal solution. Heuristics appear in situations where no algorithm exists.</p> <h3 data-number="2.3.1" id="incorrectness"><span class="header-section-number">2.3.1</span> Incorrectness</h3> <p>It is often easier to demonstrate that an heuristic is incorrect than correct.</p> <p><span><strong>Counter examples</strong></span> are a specific input to an algorithm that gives an incorrect output.</p> <p>Counter examples should be verifiable and simple. It should be easy to explain why the counter example fails and what the correct output should have been.</p> <p>Useful skills in finding counter examples:</p> <ul> <li><p>Small test inputs.</p></li> <li><p>Exaustive test inputs.</p></li> <li><p>Test assumptions.</p></li> <li><p>Test for ties.</p></li> <li><p>Test for corner cases.</p></li> </ul> <p>Beware that failing to prove incorrectness does not imply correctness.</p> <h3 data-number="2.3.2" id="induction"><span class="header-section-number">2.3.2</span> Induction</h3> <p>Induction is a fairly simple tool to help prove the correctness of iterative and recursive algorithms.</p> <p>In induction, we assume correctness for case <span class="math inline">\\(k\\)</span> and then prove correctness for case <span class="math inline">\\(k+1\\)</span>.</p> <h3 data-number="2.3.3" id="summation"><span class="header-section-number">2.3.3</span> Summation</h3> <p><span class="math inline">\\(\\sum\\)</span> is often used in algorithm analysis.</p> <p>Two types of summations cover a large number of algorithm analysis.</p> <ul> <li><p><span><strong>Arithmetic progressions</strong></span> have changing bases with a constant exponent.</p></li> </ul> <p><span class="math display">\\[\\sum_{i}^{n} i^p = \\Theta(n^{p+1})\\]</span></p> <ul> <li><p><span><strong>Geometric series</strong></span> have changing exponents with a constant base.</p></li> </ul> <p><span class="math display">\\[\\sum_{i}^{n} a^i\\]</span></p> <h2 data-number="2.4" id="modeling"><span class="header-section-number">2.4</span> Modeling</h2> <p>Modeling the problem is necessary to see it in its general form.</p> <p>Modeling a specific instance of a problem may reveal the problem to be a familiar graph problem or a tree.</p> <p>There are a few useful structures that can help model a problem:</p> <ul> <li><p>Trees</p></li> <li><p>Graphs</p></li> <li><p>Points</p></li> <li><p>Polygons</p></li> <li><p>Strings</p></li> </ul> <h1 data-number="3" id="analysis"><span class="header-section-number">3</span> Analysis</h1> <p>The most common form of analysis is to talk about an algorithm’s complexity.</p> <h2 data-number="3.1" id="complexity"><span class="header-section-number">3.1</span> Complexity</h2> <p>We use the Big O notation to compare algorthm performance.</p> <p>We only care about three cases:</p> <ul> <li><p>Worst case. Represented by <span class="math inline">\\(O(g(n))\\)</span>.</p></li> <li><p>Average case. Represented by <span class="math inline">\\(\\Theta(g(n))\\)</span></p></li> <li><p>Best case. Represented by <span class="math inline">\\(\\Omega(g(n))\\)</span></p></li> </ul> <p>There are a few different classes of complexity that cover most algorithms.</p> <ul> <li><p>Constant - <span class="math inline">\\(1\\)</span></p></li> <li><p>Logarithmic - <span class="math inline">\\(log(n)\\)</span></p></li> <li><p>Linear - <span class="math inline">\\(n\\)</span></p></li> <li><p>Superlinear - <span class="math inline">\\(n*log(n)\\)</span></p></li> <li><p>Quadratic - <span class="math inline">\\(n^2\\)</span></p></li> <li><p>Cubic - <span class="math inline">\\(n^3\\)</span></p></li> <li><p>Exponential - <span class="math inline">\\(c^n\\)</span></p></li> <li><p>Factorial - <span class="math inline">\\(n!\\)</span></p></li> </ul> <p>The relation between the complexity above can be thought of as follows:</p> <p><span class="math display">\\[n! \\gg 2^n \\gg n^3 \\gg n^2 \\gg n*log(n) \\gg n \\gg log(n) \\gg 1\\]</span> Finding the complexity of combinations of algorithms or just the combination of functions requires adding or multiplying functions.</p> <h3 data-number="3.1.1" id="addition"><span class="header-section-number">3.1.1</span> Addition</h3> <p>The complexity of the result of two added functions is the complexity of the most expensive function.</p> <p>This is to say that the Big Oh of <span class="math inline">\\(2^n + 3n + 4\\)</span> is <span class="math inline">\\(O(2^n)\\)</span> or exponential even though it contains a linear and a constant element.</p> <h3 data-number="3.1.2" id="multiplying"><span class="header-section-number">3.1.2</span> Multiplying</h3> <p>Multiplying functions with constants results in the same complexity, i.e. <span class="math inline">\\(c * n = O(n)\\)</span> for all positive values of <span class="math inline">\\(c\\)</span>.</p> <p>The result of multiplying functions by other functions makes the result have a complexity analogous to the product of the functions. For example:</p> <p><span class="math display">\\[f(n) * g(s) = h(t)\\]</span> <span class="math display">\\[O(h(t)) = O(f(n) * g(s))\\]</span></p> <h2 data-number="3.2" id="logarithms"><span class="header-section-number">3.2</span> Logarithms</h2> <p>Logarithms are an important tool for analysing complexity. It’s difficult to get an intuition for logs but remember that they are just constructs to help solve equations with exponents.</p> <p>Here are the log rules:</p> <p><span class="math display">\\[b^x = y \\Longleftrightarrow log_by = x\\]</span> <span class="math display">\\[ln x \\Longleftrightarrow log_ex\\]</span> <span class="math display">\\[log_a(x) + log_a(y) = log_a(xy)\\]</span> <span class="math display">\\[log_a(x) - log_a(y) = log_a(\\frac{x}{y})\\]</span> <span class="math display">\\[log_a(b) = \\frac{log_c(b)}{log_c(a)}\\]</span></p> <h1 data-number="4" id="data-structures"><span class="header-section-number">4</span> Data Structures</h1> <p>Data structures are what allow a program to store information in some format. Different data structures are made for different purposes.</p> <p>Using an appropriate data structure could dramatically speed up a program.</p> <p>There are two main types of data structures: contiguous and linked.</p> <ul> <li><p><span><strong>Contiguous</strong></span> data structures are stores as one slab of memory. An example of these are arrays.</p></li> <li><p><span><strong>Linked</strong></span> data structures are isolated pieces of memory joined by pointers. An example of these are linked lists.</p></li> </ul> <h2 data-number="4.1" id="arrays"><span class="header-section-number">4.1</span> Arrays</h2> <p>Arrays are the poster boy for contiguous data structures.</p> <p>We can easily allow indexing into an array because we can index in constant time using pointer arithmetic.</p> <p>Arrays also allow <span><strong>space efficiency</strong></span> because all the data is stored as just one large block and no extra memory is wasted.</p> <p>Arrays have <span><strong>memory locality</strong></span> because all the data is close to each other. This allows the processor to speed up execution by using the cache.</p> <p>Arrays are not good for dynamic length because there is no easy way to consistently make an array longer in execution.</p> <p>But <span><strong>dynamic arrays</strong></span> are actually possible by creating a new array, double the size of the previous one, and copying all the previous data to the new array if you run out of space. The disadvantage with this approach is that sometimes indexing into an array may take longer than constant time.</p> <h2 data-number="4.2" id="lists"><span class="header-section-number">4.2</span> Lists</h2> <p>Because of their nature, to talk about lists in detail, we must talk about implementing them in <code>C</code>.</p> <p>Lists are created by pointers. A list in <code>C</code> may be created with the following struct:</p> <pre><code>typedef struct list {\n    uint8 item;\n    list *next;\n}</code></pre> <div class="center"> <p>A List in <code>C</code></p> </div> <p>This <code>struct</code> allows you to store a <code>uint8 item</code> and a pointer to the next item. This was you can easily transverse an entire list by following the <code>next</code> pointer.</p> <p>Now we just need to define where a list starts and ends. If we store a pointer called <code>head</code> that points to the start of the list, we can easily access the list. As for the end, we can have the <code>next</code> pointer point to <code>NULL</code> for the last item. We could also store a pointer to the last item for convenience.</p> <p>Because lists are independent pieces of memory linked with pointers, we can easily make a list longer or shorter.</p> <p>To make a list longer, we would simple allocate a new list item using the <code>struct</code> above and then attach it to the last item in the list.</p> <p>To make a list shorter, we can remove any item from the end very easily and then free the memory. We can also remove any item in between but that’s slightly more tricky.</p> <p>For these advantages, lists do have their downsides:</p> <ul> <li><p>Lists use more memory to store data than an array.</p></li> <li><p>Lists take longer to transverse than an array.</p></li> <li><p>Lists may not be cache optimized on allocation.</p></li> </ul> <p>The <code>struct</code> above implements a <span><strong>singly-linked</strong></span> list. We can have <span><strong>doubly-linked</strong></span> lists as well. A doubly-linked list has pointer to the next and the previous item.</p> <h2 data-number="4.3" id="stacks"><span class="header-section-number">4.3</span> Stacks</h2> <p>Stacks can be thought of as a stack of plates. We can put more and more plates on top but we can’t remove the 5th plate from the top without removing the top four first.</p> <p>For this reason, we consider stacks to be a LIFO data structure. <span><strong>LIFO</strong></span> stands for last-in first-out. The last item to be put on the stack must be the first item to be pulled off.</p> <h2 data-number="4.4" id="queues"><span class="header-section-number">4.4</span> Queues</h2> <p>Queues are very similar to stacks but with one key difference. Queues are FIFO. <span><strong>FIFO</strong></span> stands for first-in first-out.</p> <p>An example of these might be a job list. The first job on the list has to be completed first.</p> <h2 data-number="4.5" id="dictionaries"><span class="header-section-number">4.5</span> Dictionaries</h2> <p>A real dictionary holds a definition for every word. The dictionary data structure holds a key and a corresponding value for each key.</p> <p>We can perform many operations on a dictionary:</p> <ul> <li><p>Search</p></li> <li><p>Insert</p></li> <li><p>Delete</p></li> <li><p>Max/Min</p></li> <li><p>Predecessor/Successor</p></li> </ul> <p>The complexity of these operations depends on the implementation of the dictionary.</p> <h3 data-number="4.5.1" id="criteria"><span class="header-section-number">4.5.1</span> Criteria</h3> <p>Before we discuss implementations, we must create a criteria to compare them all on.</p> <p>The five items we’ll use for dictionaries are:</p> <ul> <li><p><span><strong>Search</strong></span></p> <ul> <li><p>The time it takes to search for a specific item.</p></li> </ul></li> <li><p><span><strong>Insertion</strong></span></p> <ul> <li><p>The time it takes to insert a new item.</p></li> </ul></li> <li><p><span><strong>Deletion</strong></span></p> <ul> <li><p>The time it takes to delete an item but the item’s location is known in a list or unsorted structure.</p></li> </ul></li> <li><p><span><strong>Max/Min</strong></span></p> <ul> <li><p>The time it takes to find the max or min element.</p></li> </ul></li> <li><p><span><strong>Pred/Succ</strong></span></p> <ul> <li><p>The time it takes to return the next or last item in a sequence, e.g. <code>succ 8 = 9</code>. Note that we know the location/value of the initial item.</p></li> </ul></li> </ul> <h3 data-number="4.5.2" id="array"><span class="header-section-number">4.5.2</span> Array</h3> <p>We can use an array to create a dictionary. But we have two options: sorted or unsorted array.</p> <div class="center"> <table> <caption>Dictionary Array Implementations</caption> <thead> <tr class="header"> <th style="text-align:left">Operation</th> <th style="text-align:left">Unsorted</th> <th style="text-align:left">Sorted</th> </tr> </thead> <tbody> <tr class="odd"> <td style="text-align:left">Search</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(log(n))\\)</span></td> </tr> <tr class="even"> <td style="text-align:left">Insertion</td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> </tr> <tr class="odd"> <td style="text-align:left">Deletion</td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span> or <span class="math inline">\\(O(n)\\)</span><span><strong></strong></span></td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> </tr> <tr class="even"> <td style="text-align:left">Max/Min</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> </tr> <tr class="odd"> <td style="text-align:left">Pred/Succ</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(o(1)\\)</span></td> </tr> </tbody> </table> </div> <p>In an unsorted array:</p> <ul> <li><p>Search takes linear time because we need to compare each item.</p></li> <li><p>Insertion takes constant time because we can just append new data. This assumes that we have space at the end of the array.</p></li> <li><p><span><strong></strong></span>Deletion can take constant or linear time.</p> <ul> <li><p>It takes constant time if we delete an item and then replace its place with the last item and store the length of the array.</p></li> <li><p>It takes linear time if we decide to move all elements up to fill a hole left by deletion.</p></li> </ul></li> <li><p>Max/Min both take linear time because we need to do a full-scan of the array.</p></li> <li><p>Pred/Succ in an unsorted array take linear time. The easiest way to see this is to thing of Pred as finding the min after removing a min. This means that we find the min twice and remove an item once. That is still a linear operation.</p></li> </ul> <p>In a sorted array:</p> <ul> <li><p>Search takes logarithmic time due to binary search.</p></li> <li><p>Insertion takes linear time because we have to find the right spot to insert the item and then shift all the items to make space. Finding the right spot is logarithmic but shifting the items is linear so the whole operation is linear.</p></li> <li><p>Deletion takes linear time with similar reasoning to insertion.</p></li> <li><p>Max/Min take constant time because we just use the first or last item.</p></li> <li><p>Pred/Succ takes constant time because we just return the next or previous item in the array.</p></li> </ul> <h3 data-number="4.5.3" id="lists-1"><span class="header-section-number">4.5.3</span> Lists</h3> <p>We can also use lists to create a dictionary but this has its own trade-offs.</p> <p>Note that for deletion, the item to be deleted is passed in as an argument.</p> <div class="center"> <table> <caption>Dictionary Singly-Linked List Implementations</caption> <thead> <tr class="header"> <th style="text-align:left">Operation</th> <th style="text-align:left">Unsorted</th> <th style="text-align:left">Sorted</th> </tr> </thead> <tbody> <tr class="odd"> <td style="text-align:left">Search</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> </tr> <tr class="even"> <td style="text-align:left">Insertion</td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> </tr> <tr class="odd"> <td style="text-align:left">Deletion</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> </tr> <tr class="even"> <td style="text-align:left">Max/Min</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> </tr> <tr class="odd"> <td style="text-align:left">Pred/Succ</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> </tr> </tbody> </table> </div> <p>For a singly-linked unsorted list:</p> <ul> <li><p>Search takes linear time because we compare every item.</p></li> <li><p>Insertion takes constant time because we can just append an item.</p></li> <li><p>Deletion takes linear time because we need to fix the <code>pred</code> of the deleted item which takes linear time to find.</p></li> <li><p>Max/Min take linear time because we need to search the entire list.</p></li> <li><p>Pred/Succ take linear time for the same reasons as an unsorted list.</p></li> </ul> <p>For a singly-linked sorted list:</p> <ul> <li><p>Search takes linear time because we still need to run through each item.</p></li> <li><p>Insertion takes linear time to find the right spot.</p></li> <li><p>Deletion takes linear time to fix the <code>pred</code> item.</p></li> <li><p>Max/Min take constant time because we just return the <code>head</code> and <code>tail</code> of the list.</p></li> <li><p>Pred take linear time because it is a singly-linked list. Succ only takes constant time.</p></li> </ul> <div class="center"> <table> <caption>Dictionary Doubly-Linked List Implementations</caption> <thead> <tr class="header"> <th style="text-align:left">Operation</th> <th style="text-align:left">Unsorted</th> <th style="text-align:left">Sorted</th> </tr> </thead> <tbody> <tr class="odd"> <td style="text-align:left">Search</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> </tr> <tr class="even"> <td style="text-align:left">Insertion</td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> </tr> <tr class="odd"> <td style="text-align:left">Deletion</td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> </tr> <tr class="even"> <td style="text-align:left">Max/Min</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> </tr> <tr class="odd"> <td style="text-align:left">Pred/Succ</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> </tr> </tbody> </table> </div> <p>For a doubly-linked unsorted list:</p> <ul> <li><p>Search takes linear time because we still transverse the list.</p></li> <li><p>Insertion takes constant time because we just append the item.</p></li> <li><p>Deletion takes constant time because fixing the previous item is constant now.</p></li> <li><p>Max/Min take linear time to transverse the list.</p></li> <li><p>Pred/Succ take linear time similar to an unsorted array.</p></li> </ul> <p>For a doubly-linked sorted list:</p> <ul> <li><p>Search takes linear time to actually transverse the array.</p></li> <li><p>Insertion takes linear time to find the right place to insert.</p></li> <li><p>Deletion takes constant time because a pointer to the item to be deleted is passed in and constant time to fix the pointers.</p></li> <li><p>Max/Min take constant time because we just need the first and last item.</p></li> <li><p>Pred/Succ take constant time because this is a doubly-linked list allowing easy access to the previous items.</p></li> </ul> <h2 data-number="4.6" id="binary-search-trees"><span class="header-section-number">4.6</span> Binary Search Trees</h2> <p>Binary search trees allow us to do fast search and fast update.</p> <p>A binary search tree is just a structure with two branches coming out of it. We can call the current node the parent. The parent’s left branch has items lower than parent and the parent’s right branch has items higher than parent.</p> <p>We can implement a BST (binary search tree) in <code>C</code> using the following struct:</p> <pre><code>typedef struct tree {\n    uint8 value;\n    struct tree *left_node;\n    struct tree *right_node;\n    struct tree *parent_node;\n}</code></pre> <div class="center"> <p>A Tree in <code>C</code></p> </div> <p>Note that we also store a pointer to the parent.</p> <p>Because of the nature of BSTs, operations are very quick.</p> <div class="center"> <table> <caption>BSTs</caption> <thead> <tr class="header"> <th style="text-align:left">Operation</th> <th style="text-align:left">BST</th> </tr> </thead> <tbody> <tr class="odd"> <td style="text-align:left">Search</td> <td style="text-align:left"><span class="math inline">\\(O(h)\\)</span></td> </tr> <tr class="even"> <td style="text-align:left">Insertion</td> <td style="text-align:left"><span class="math inline">\\(O(h)\\)</span></td> </tr> <tr class="odd"> <td style="text-align:left">Deletion</td> <td style="text-align:left"><span class="math inline">\\(O(h)\\)</span></td> </tr> <tr class="even"> <td style="text-align:left">Max/Min</td> <td style="text-align:left"><span class="math inline">\\(O(h)\\)</span></td> </tr> <tr class="odd"> <td style="text-align:left">Pred/Succ</td> <td style="text-align:left"><span class="math inline">\\(O(h)\\)</span></td> </tr> </tbody> </table> </div> <p>Note that the <span class="math inline">\\(h\\)</span> in <span class="math inline">\\(O(h)\\)</span> refers to the tree’s height. A perfectly balanced tree with an equal number of items on the left and right is shorter than an unbalanced tree. In a perfectly balanced tree, <span class="math inline">\\(h = log(n)\\)</span> where <span class="math inline">\\(n = number of items\\)</span>.</p> <ul> <li><p>Search takes <span class="math inline">\\(O(h)\\)</span> time because we search through the tree by making a choice at each branch. On a perfectly balanced tree, we essentially perform binary search.</p></li> <li><p>Insert takes <span class="math inline">\\(O(h)\\)</span> time because we just go all the way down the height of the tree.</p></li> <li><p>Deletion takes <span class="math inline">\\(O(h)\\)</span> time to fix the BST after deleting the item.</p></li> <li><p>Max/Min take <span class="math inline">\\(O(h)\\)</span> time because we just go the the left-most item for min and the right-most item for max.</p></li> <li><p>Transversal takes linear time.</p></li> </ul> <p>There are implementations of BSTs that will maintain the tree to be as close to a perfectly balanced tree after any operation. These have a slightly larger overhead on operations.</p> <p>As for implementations, most of the operations are fairly simple. Deletion is the tricky one. Deleting an item from a BST means that there are a few different cases:</p> <ol> <li><p>Deleting an item with no children is simple.</p></li> <li><p>Deleting an item with only one children is still simple.</p></li> <li><p>Deleting an item with two children is tricky. In this case, the correct method is to find the min of the right sub-tree of the parent that will be deleted.</p></li> </ol> <h3 data-number="4.6.1" id="traversal"><span class="header-section-number">4.6.1</span> Traversal</h3> <ul> <li><p>Pre-order</p> <ul> <li><p>Root is visited first, then left, then right.</p></li> </ul></li> <li><p>In-order</p> <ul> <li><p>Left is visited first, then root, then right.</p></li> </ul></li> <li><p>Post-order</p> <ul> <li><p>Left is visited first, then right, then root.</p></li> </ul></li> </ul> <h2 data-number="4.7" id="priority-queues"><span class="header-section-number">4.7</span> Priority Queues</h2> <p>Priority queues are data structures that work based on the value in a key, value pair. The idea is that while a regular queue is FIFO, a priority queue only cares about your priority.</p> <p>Items are processed based on their priority instead of when they were inserted. Priority queues can be found in a lot of places. Waiting times in a clinic may be a form of priority queues.</p> <p>A priority queue has a few basic operations:</p> <ul> <li><p>Insert.</p></li> <li><p>Search max/min.</p></li> <li><p>Delete max/min.</p></li> </ul> <p>There are a few different ways to implement a priority queue.</p> <div class="center"> <table> <caption>BSTs</caption> <thead> <tr class="header"> <th style="text-align:left">Operation</th> <th style="text-align:left">Unsorted Array</th> <th style="text-align:left">Sorted Array</th> <th style="text-align:left">Balanced BST</th> </tr> </thead> <tbody> <tr class="odd"> <td style="text-align:left">Insertion</td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(h)\\)</span></td> </tr> <tr class="even"> <td style="text-align:left">Search max/min</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(1)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(h)\\)</span></td> </tr> <tr class="odd"> <td style="text-align:left">Deletion max/min</td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(n)\\)</span></td> <td style="text-align:left"><span class="math inline">\\(O(h)\\)</span></td> </tr> </tbody> </table> </div> <p>For an unsorted array:</p> <ul> <li><p>Insert is constant.</p></li> <li><p>Searching for max/min is linear unless you keep a pointer to the minimum or maximum element.</p></li> <li><p>Deleting max/min is linear because you need to search for a new min/max.</p></li> </ul> <p>For a sorted array:</p> <ul> <li><p>Insert in linear.</p></li> <li><p>Searching for max/min is constant if you know the size of the array or have a pointer to the max element. Searching for min is always constant.</p></li> <li><p>Deleting min/max is constant as long as you do not have to fix holes.</p></li> </ul> <p>For a balanced tree:</p> <ul> <li><p>Insert is logarithmic because we just need to go down the balanced tree.</p></li> <li><p>Searching for max/min is constant.</p></li> <li><p>Deleting max/min is logarithmic because we fix the tree down.</p></li> </ul> <h2 data-number="4.8" id="hash-functions"><span class="header-section-number">4.8</span> Hash Functions</h2> <p>The <span><strong>hash function</strong></span> is a mathematical function that outputs an identifier for an input. Complex hash functions like the md5 create unique outputs for any input. All hash functions are math functions, i.e. a specific input will always have the same output.</p> <h3 data-number="4.8.1" id="hash-tables"><span class="header-section-number">4.8.1</span> Hash Tables</h3> <p>Hashtables are a data structure that allow very quick searching and updating. Updating compelxity depends on implementation.</p> <p>Hashtables use a hash function to get an identifier or key for any item to store. This key may be reduced or lowered by using the modulus function. The final key can be thought of as a specific bucket number in a large row of buckets.</p> <p>An example might be student IDs. Our very simple hashfunction can be a function that returns the last two digits of an ID, i.e. <span class="math inline">\\(f(12345) = 45\\)</span>. But this means we need 100 buckets because the last two digits of an ID go from 0 to 99. To further reduce the number of digits, we can use the modulus operation. If we do <span class="math inline">\\(45 mod 10 = 5\\)</span>, then we could only get numbers from 0 to <span class="math inline">\\(10-1 = 9\\)</span>.</p> <p>But we must have more than 10 students. What happens if two students end up with the same key?</p> <p>To solve this issue, we use <span><strong>chaining</strong></span>. Each bucket has a list (or array, etc.) of items that contains all the items that got matched to that bucket. If two student do end up with the same key, they will both be inserted into the list under that bucket.</p> <p>Note that when two different items get matched to the same bucket, we call it a <span><strong>collision</strong></span>.</p> <h3 data-number="4.8.2" id="other-uses"><span class="header-section-number">4.8.2</span> Other Uses</h3> <p>Hash functions are very useful in many different areas apart from just creating hashtables.</p> <p>They can be applied to most situations that require comparing large pieces of data. The hash function can be used to get a unique hash code of the item we are searching for. Then, it is compared to the hash codes of the items that we are searching.</p> <p>A specific example of hashing is checking file integrity. Let’s say a company wants to ensure that a file sent in an email was not tampered with, they can give the file and its hash code. Then you could calculate the hash code yourself and compare it to what it should be. This is the basis for asymmetric cryptography.</p> <h1 data-number="5" id="sorting"><span class="header-section-number">5</span> Sorting</h1> <p>Sorting may be the most studied problem in computer science. There are many algorithms to sort because computers spend a lot of time just sorting things.</p> <p>Sorting algorithms are often the basis for other, more complicated algorithms. Different sorting algorithms have different strengths and are designed for different tasks.</p> <p>Sorting can be a precursor to many different tasks:</p> <ul> <li><p>Sorting can be used as a step to searching. Binary search can be applied after sorting to yield an <span class="math inline">\\(n log(n)\\)</span> for searching an unsorted data struct.</p></li> <li><p>Sorting can be used to get item frequency in a list.</p></li> <li><p>Neighbouring items can be compared for uniqueness. It can be used to get uniqueness at the same time.</p></li> <li><p>We can check if two strings are anagrams by sorting them and comparing them.</p></li> </ul> <p>There are many more applications that sorting can make efficient.</p> <h2 data-number="5.1" id="selection-sort"><span class="header-section-number">5.1</span> Selection Sort</h2> <p>The gist of selection-sort is simple. We run through all the items in an array left to right. As we go along, we find the minimum of the items to the right of our current item. If this minimum is less than our current item, then we swap the current item and the minimum.</p> <p>All we’re really doing is finding minimum after minimum. That’s why naive selection-sort runs in <span class="math inline">\\(O(n^2)\\)</span>. It finds the minimum linearly. selection-sort is also an in-place sorting algorithm.</p> <p>If we could speed up finding minimum, we could increase selection-sort dramatically. Luckily, there are data structures that allow you to quickly get the minimum like heaps. This is discussed in the heapsort section.</p> <h2 data-number="5.2" id="insertion-sort"><span class="header-section-number">5.2</span> Insertion Sort</h2> <p>Insertion sort is also fairly simple. We run through the entire array to sort. As we go along, we are dividing the array into two: sorted and untouched. The divider between the two is just the current item. Everything to the left of current item is sorted and to the right is untouched.</p> <p>The sorted section is sorted in this method: Each iteration, we grab the current item and continuously swap it with the item to the left until it is in the sorted position.</p> <p>Eventually, everything is sorted.</p> <p>Insertion sort runs in <span class="math inline">\\(O(n^2)\\)</span> and is an in-place sorting algorithm.</p> <h2 data-number="5.3" id="heapsort"><span class="header-section-number">5.3</span> Heapsort</h2> <p>To understand heapsort we have to understand selection-sort first. This is because heapsort is just a sped-up selection-sort.</p> <h3 data-number="5.3.1" id="heaps"><span class="header-section-number">5.3.1</span> Heaps</h3> <p>The trick to speeding up selection-sort is noticing that it takes linear time to find the minimum in each iteration. If we could utilize other data structures that allow quickly finding the min, we could speed up selection-sort dramatically.</p> <p>We’ve already discussed binary search trees which we could use to solve this problem since they allow finding min/max in <span class="math inline">\\(O(h)\\)</span> time. Similarly, we could use a priority queue based on a binary search tree.</p> <p>But we can also use something called heaps.</p> <p>Heaps are trees but they’re not quite binary search trees. They have no order apart from one rule. The parent must be a greater value than its children in a <span><strong>max-heap</strong></span> and the parent must be smaller than its children in a <span><strong>min-heap</strong></span>.</p> <p>This means that finding an item in the tree will take linear time. Unless we’re finding the min in a min-heap or max in a max-heap, then it will be constant time.</p> <p>Heaps are also created without pointers, unlike BSTs. We can represent a heap in an array. We can do this because we can find the parent using a formula since the way a heap fills up is entirely predictable.</p> <p>We know that a heap has <span class="math inline">\\(\\le 2^{levels}\\)</span> items because it’s a binary tree. We know that the left child of an item at <span class="math inline">\\(k\\)</span> is at <span class="math inline">\\(2k\\)</span> and its right child is at <span class="math inline">\\(2k+1\\)</span>. And the parent of a node <span class="math inline">\\(n\\)</span> is at <span class="math inline">\\(floor(n/2)\\)</span>. This can be a little tricky to visualize so work it out on paper.</p> <p>When loading a heap, we always fill the left child before the right child. This means that the last level of a heap will probably not be entirely filled. If we were to represent this last level entirely, then it would be a large drain on memory for large heaps. Instead, if we just implement some checks and store the heap size, we only have to have <span class="math inline">\\(n\\)</span> spaces in memory and no empty spots.</p> <p>Now for some basic operations on heaps:</p> <ul> <li><p>Search.</p></li> <li><p>Insert.</p></li> <li><p>Extract min/max.</p></li> </ul> <p><span><strong>Search</strong></span> occurs in <span class="math inline">\\(O(n)\\)</span>, as previously discussed.</p> <p><span><strong>Insert</strong></span> is fairly simple. We insert a new item at the end of our array that represents the heap. But then we need to ensure that this new item has no conflicts with its parent. If there is a conflict, we swap the parent and the newly inserted item. Now we need to call this check&amp;swap function recursively on its new position to ensure there is no conflict again.</p> <p>Extracting the <span><strong>min/max</strong></span> is a little tricky. After we extract the min, we need to find the new min as well. To do this we need to put in a new temporary root and then fix the tree going down. We can grab the last node in the tree as the new root for convenience. Then we need to check and fix the tree going down.</p> <p>To fix the tree going down, we need to remember that in the trio of a parent and its two children, the parent has a greater/lesser value than its children. And this means that if there is a conflict with the parent, we just need to swap the parent with the left or right child depending on the values. But we may not be done yet. Similarly to insertion, we need to call this fix function recursively with the old, now-swapped parent in its new position.</p> <p>That is essentially the algorithm. We start at position 0, i.e. the root, after extracting the min/max and check if the children are lower/higher. If they are not we make the lowest/greatest child the new parent and call the fix_down function on the new location of the misplaced item. This process exits when there are no conflicts with the misplaced item or we get to the bottom of the heap. Note that this process takes <span class="math inline">\\(log(h)\\)</span> time because it goes down the height of the tree.</p> <p>Lastly, note that heaps are very useful. The details discussed above can be very easily used to implement a priority queue or other structures.</p> <h3 data-number="5.3.2" id="sorting-1"><span class="header-section-number">5.3.2</span> Sorting</h3> <p>Heapsort is using heaps to quickly extract the min/max. As we’ve already discussed, extracting the min/max is constant but we need to fix the heap down so it’s a <span class="math inline">\\(log(n)\\)</span> operation overall.</p> <p>In an array of <span class="math inline">\\(n\\)</span> items, we would extract the min/max <span class="math inline">\\(n\\)</span> times so the total complexity of sorting an array would be <span class="math inline">\\(n log(n)\\)</span>.</p> <h2 data-number="5.4" id="mergesort"><span class="header-section-number">5.4</span> Mergesort</h2> <p>Mergesort is based on one of the most powerful ideas in computer science, divide-and-conquer.</p> <h3 data-number="5.4.1" id="divide-and-conquer"><span class="header-section-number">5.4.1</span> Divide and Conquer</h3> <p>The idea behind divide-and-conquer is fairly simple yet very powerful.</p> <p>Generally, smaller problems are easier to solve than larger problems. This is the idea behind divide-and-conquer. We divide up a problem into many sub-parts and then solve each one of them.</p> <p>Once the sub-parts are solved, we combine them to get a complete solved solution.</p> <h3 data-number="5.4.2" id="sorting-with-divide-and-conquer"><span class="header-section-number">5.4.2</span> Sorting with Divide-and-Conquer</h3> <p>Mergesort uses divide-and-conquer to sort.</p> <p>The idea is to recursively divide a array into many smaller arrays. Then we sort the small portions. After the small portions are sorted, we combine them back into a larger list or array.</p> <p>The simplest implemention involves dividing up an array into halves recursively until you’re left with just one item. Then, merge all these arrays with one item back together in the sorted order.</p> <p>It’s a little tricky to get a mental model of the recursion in mergesort. Just understand that by calling mergesort on the left half recursively and the right half recursively, it will work out. I find it difficult to visualize this but it is easy to walk through the steps to see why it works.</p> <h3 data-number="5.4.3" id="visualizing-mergesort"><span class="header-section-number">5.4.3</span> Visualizing Mergesort</h3> <pre><code>def mergesort(l):\n    if len(l) &lt;= 1:\n        return l\n\n    middle = len(l) // 2\n\n    left = mergesort(l[:middle])\n    right = mergesort(l[middle:])\n    full = merge(left, right)</code></pre> <div class="center"> <p>Mergesort in Python3</p> </div> <p>Nevertheless, here is an attempt to visualize it. It’s easy to see that by calling mergesort recursively on the left half, we’ll eventually be down to one item.</p> <p>Then we’ll return that one item which brings us into the scope with an array with two items. We call merge sort on the right half of the array with two items returning just the 2nd item in the array. Now we merge the two in sorted order. We have a sorted array of two items at this point.</p> <p>Now we’ll go back from this scope to the one with an array of four items. We’ve already sorted the left but we’ll call merge sort on the right which will do the whole business of calling itself until there’s only one item and eventually you’ll get another sorted array of two items.</p> <p>Now we’ve got two arrays of two items that we’ll merge together to get a sorted array of four items. This is going to continue until we end up merging the two halves of the initial, complete array yielding a final completely sorted array.</p> <h3 data-number="5.4.4" id="merging-arrays"><span class="header-section-number">5.4.4</span> Merging Arrays</h3> <p>The trickiest part of mergesort is the merging. It helps to remember that the merge step receives two arrays. All the merge step needs to do is grab items from each array in increasing/decreasing order.</p> <p>This is a simple problem. We know that each individual array is sorted already so we just need to check if the first item from array_one is less/greater than the first item from array_two. If yes, then grab the item from array_one and check again until both arrays are empty.</p> <p>When both arrays are empty you will have a new sorted array.</p> <h3 data-number="5.4.5" id="complexity-1"><span class="header-section-number">5.4.5</span> Complexity</h3> <p>As for the complexity of merge sort, we only take <span class="math inline">\\(O(log(n))\\)</span> steps to divide and it’s linear to sort the divided arrays. The overall complexity of merge sort is <span class="math inline">\\(O(n log(n))\\)</span>.</p> <h2 data-number="5.5" id="quicksort"><span class="header-section-number">5.5</span> Quicksort</h2> <p>Quicksort is generally considered to be the fastest sorting algorithm. It runs in <span class="math inline">\\(O(n log(n))\\)</span>.</p> <p>The idea behind quicksort is similar to divide-and-conquer as well. We take an array and choose a pivot point. The entire array is then divided into two sections: less than pivot, and greater than pivot.</p> <p>Now we have three total sections. <code>less_than_pivot &lt; pivot &lt; greater_than_pivot</code></p> <p>The great thing is that pivot is now in its final position in the array. Now we just need to call quicksort recursively on the two other sections of the array until all items are in their appropriate position.</p> <p>This recursion is easier to visualize than the mergesort recursion. That one is far trickier to visualize.</p> <p>The tricky part of quicksort is the partition algorithm, i.e. the part that separarates the arrays into two parts.</p> <h3 data-number="5.5.1" id="partitioning"><span class="header-section-number">5.5.1</span> Partitioning</h3> <p>The general idea of the partition function is as follows. The partition function takes in a pivot index and it returns the new pivot index after partitioning all the data.</p> <p>To partition the data, first it will swap the initial pivot point and the first item in the array. This makes things a little neater. The next part gets a little tricky.</p> <p>The goal is to separate the array into two sections: less than pivot and more than pivot. We do this by running through the array. But we keep a left hand and right hand pointers, i.e. just index trackers.</p> <p>Initially, both these pointers point to the index one, i.e. right after pivot. Now when running through the entire array, we increase the right hand every time but the left hand increases only for a special case.</p> <p>We only increase the left hand if the right hand is pointing to an item that is less than the pivot’s value (assuming ascending order). And we also swap the items at left hand and right hand in this case.</p> <p>The result is that after transversing the entire array, the only thing left is to swap the pivot with left_hand-1. The partition is now done.</p> <p>The partitioning can be a little tricky to understand in the details above because they are describing a <code>C</code> program. It’s a lot simpler to do quicksort with list comprehensions and other higher order constructs but they’re likely slower.</p> <h3 data-number="5.5.2" id="randomization"><span class="header-section-number">5.5.2</span> Randomization</h3> <p>Quicksort’s complexity is a little tricky to understand. This is because its speed depends on how efficient partition is. If partition always divides the array fairly equally, the algorithm runs quickly. But if partition divides the array into one item and the rest of the array, then quicksort will be very slow.</p> <p>This variation in complexity is why the notion of worst case, best case, and average case exists. The average case for quicksort is likely <span class="math inline">\\(O(nlog(n))\\)</span> because we don’t need the pivot to be perfectly in the middle every time. It still works by being close enough.</p> <p>The selection of the pivot point is what controls the speed of quicksort. One way to select it is to just pick the middle of the array. Another is to select the first item or the last item.</p> <p>If the pivot is always the first item or the last item and the array passed in is sorted or almost sorted, then quicksort will be very slow because partition will never partition efficiently.</p> <p>A solution to this is to randomize the input array before processing. This always ensure that sorted data cannot slow down quicksort.</p> <p>Another solution is to <span><strong>random</strong></span>ly select the pivot point at each step. Randomization can be and is used a lot for cases like selecting the pivot point. Randomized data has a tendency to usually be the average case which helps make our quicksort consistent.</p> <p>If we select our pivot point randomly and randomize the input data, we can expect <span class="math inline">\\(O(n log(n))\\)</span> performace most of the time. It will also be a little faster than other sorting algorithms.</p> <h2 data-number="5.6" id="bucketsort"><span class="header-section-number">5.6</span> Bucketsort</h2> <p>Bucketsort is similar to the buckets analogy in hashtables.</p> <p>Bucketsort uses the idea of divide-and-conquer but in a slightly different way than normal. Bucketsort utilizes more space than most sorting algorithms.</p> <p>The idea is that before sorting we just place items into ordered buckets. The buckets are known so it takes <span class="math inline">\\(O(n)\\)</span> to iterate through the entire list and place in buckets. concatanate all buckets together.</p> <p>The complexity of bucketsort is a tricky to define. The reason for this is that if the distribution among the buckets is sparse, then we just waste time bucketing items.</p> <p>Bucketsort works fastest if there are lots of buckets and a fairly even distribution. This is because we get lots of already ordered buckets with only a few items to actually sort.</p> <h2 data-number="5.7" id="intricacies"><span class="header-section-number">5.7</span> Intricacies</h2> <p>There are a number of details and choices that we must be aware of when talking about sorting algorithms.</p> <h3 data-number="5.7.1" id="choices"><span class="header-section-number">5.7.1</span> Choices</h3> <p>Most sorting algorithms allow you to decide whether to sort in ascending or descending order. This is usually done with the help of a comparison that the user can specify.</p> <p>For key, value pairs, sometimes you may want the sort the keys only. Maybe you want to sort the values based on the keys or other combinations. All these can be implemented into an algorithm.</p> <h3 data-number="5.7.2" id="special-cases"><span class="header-section-number">5.7.2</span> Special Cases</h3> <p>Apart from choices, sometimes you need to be aware of special cases. One special case is the equal case. What to do with items that have equal value?</p> <p>Another problem is specific instances of a general problem in sorting. For instance, strings are usually repressented as a list. They could be sorted in a way very similar to lists but you need to decide to compare ascii values for letters or implementing an alphabet heriarchy somehow.</p> <h1 data-number="6" id="graphs"><span class="header-section-number">6</span> Graphs</h1> <p>Graphs are an incredibly useful concept in computer science. They can be found almost everywhere.</p> <p>A graph is just a representation of a connected system. In a graph, there are <span><strong>vertices</strong></span> and there are <span><strong>edges</strong></span> connecting the vertices. A node can be anything. It could be intersections on a map, possible moves in chess, etc. The edges represent a relation by connecting two vertices.</p> <p>Graphs can be used to model a wide variety of problems.</p> <p>Generally speaking, a graph is represented with two items: a list of <span><strong>vertices</strong></span> and a list of <span><strong>edges</strong></span>. Edges are just a tuple containing two vertices.</p> <h2 data-number="6.1" id="types-and-terminology"><span class="header-section-number">6.1</span> Types and Terminology</h2> <p>Because graphs can be used to model a wide variety of situations, they also come in many different types for more accurate modeling.</p> <p>A <span><strong>directed graph</strong></span> has one-way edges, i.e. the relation is a one-way relation much like some roads are one-way. An <span><strong>undirected graph</strong></span> has two-way edges. Relations work both ways. Note that directed graphs can also have two-way edges but they need to be explicitly specified. But undirected graphs cannot have one-way edges.</p> <p>A <span><strong>weighted graph</strong></span> assigns a value to each edge. This means there is a cost to each relation. An <span><strong>unweighted graph</strong></span> has no cost or value assigned to any edge. Note that sometimes vertices are given weights rather than the edges but we can just calculate the edge weight using the node weights.</p> <p>A <span><strong>non-simple graph</strong></span> contains edges that contain only one vertex called a <span><strong>self-loop</strong></span>. They can also contain edges that occur more than once. These are called <span><strong>multiedges</strong></span>. A <span><strong>simple graph</strong></span> contains neither.</p> <p>A <span><strong>sparse graph</strong></span> contains relatively fewer edges than are theoretically possible. A <span><strong>dense graph</strong></span> contains far more edges. There is no formal distinction between a sparse and dense graph.</p> <p>A <span><strong>cyclic graph</strong></span> contains cycles whereas a <span><strong>acyclic graph</strong></span> does not contain cycles. Directed acyclic graphs are called <span><strong>DAG</strong></span>s.</p> <p>An <span><strong>embedded graph</strong></span> contains geometric information that is useful to the graph. An example of this is a geographic map. A <span><strong>topological graph</strong></span> contains no geometric relations. The absolute position of the vertices do not matter.</p> <p>An <span><strong>implicit graph</strong></span> is built as you use them. <span><strong>Explicit graphs</strong></span> are build explicitly.</p> <p>A <span><strong>labeled graph</strong></span> contains unique identifiers for vertices. An <span><strong>unlabeled graph</strong></span> does not.</p> <p>The <span><strong>degree</strong></span> of a vertex is the number of edges that connect to it.</p> <h2 data-number="6.2" id="searching-a-graph"><span class="header-section-number">6.2</span> Searching a Graph</h2> <p>Searching and transversing a graph is the most useful function of a graph.</p> <p>For the purpose of discussion, assume that a graph is represented with a dictionary that uses a vertex label as a key and a list of neighbours from that vertex as its values. Note that the list of neighbours is usually called an <span><strong>adjacency list</strong></span>.</p> <p>Searching through this graph is relatively similar for the two primary methods: breadth-first search and depth-first search.</p> <h3 data-number="6.2.1" id="breadth-first-search"><span class="header-section-number">6.2.1</span> Breadth-First Search</h3> <p>Breadth-first search focuses on searching widely first. The idea is best illustrated with an example. Let’s say you lost your remote in the living room and now you need to search for it.</p> <p>There is an implicit, embedded graph here. The furniture, carpet, walls, etc. are all vertices and any relations between them are the edges.</p> <p>We could search for the remote by looking at the closest places first. Is it on the floor in front of you? Is it right behind you? Are you sitting on it?</p> <p>If this doesn’t work, we can look a little farther. Let’s check the sofa cushions. We can check under the table and sofa.</p> <p>If we still can’t find it, let’s look even farther away. Check the other rooms in the house. Hopefully, by now you’ve found the remote.</p> <p>Searching for that remote was an example of breadth-first search. We looked in the vertices closest to us first and expanded our search as if it was a growing circle.</p> <h3 data-number="6.2.2" id="depth-first-search"><span class="header-section-number">6.2.2</span> Depth-First Search</h3> <p>We can use the same example to illustrate depth-first search. Let’s lose our remote again.</p> <p>This time, we check the sofa to the left for the remote. Then we check the floor behind the sofa. But it’s not there so we go into the room to the left and check the floor there.</p> <p>It doesn’t seem to be there so we come back to the sofa and search the table in front of us. Then we go to the room in front of us. And we find the remote in that room.</p> <p>This was an example of depth-first search. Note how our search for focused on following a trail deeper and deeper rather than checking our immediate surroundings first.</p> <h3 data-number="6.2.3" id="implementation"><span class="header-section-number">6.2.3</span> Implementation</h3> <p>Both of these searches are very similar in their implementation. They only have one difference between them.</p> <h4 data-number="6.2.3.1" id="breadth-first-search-1"><span class="header-section-number">6.2.3.1</span> Breadth-First Search</h4> <p>First let’s discuss breadth-first search.</p> <p>Let’s grab that graph that stores its data in a dictionary as described in the "Searching a Graph" section.</p> <p>We need a starting vertex called a root. We’ll also keep track of which vertex we’ve already been to. Sometimes, we also keep track of where the vertex we arrived at another vertex from.</p> <p>Now all we do is get the neighbors of our root and add them all to a queue. Then we grab an item off the queue and check if we’ve visited it before. If not, then get its neighbors and add them to the queue. If we have visited it before, we just throw it out.</p> <p>We continue this process until our queue is empty at which point all the neighbours of all the vertices, i.e. all vertices, have been processed.</p> <p>To understand why this is breadth-first search picture a simple graph, and use the algorithm on it. You’ll notice that the queue (FIFO) is the key here. The queue allows you to expand your search outward like an circle with an increasing radius.</p> <h4 data-number="6.2.3.2" id="depth-first-search-1"><span class="header-section-number">6.2.3.2</span> Depth-First Search</h4> <p>That queue is also the key to understanding the difference between breadth-first search and depth-first search.</p> <p>The implementation for depth-first search is identical apart from one detail. Instead of using a queue, we use a stack. So now instead of FIFO we have LIFO. This makes the search go deep following trails as far as they go.</p> <p>Note that although our algorithm would stop when the queue or stack is empty, that does not necessarily mean that all the vertices in a graph have been processed. But we know that all the connected vertices in that section of the graph have been processed.</p> <h3 data-number="6.2.4" id="more-terminology"><span class="header-section-number">6.2.4</span> More Terminology</h3> <p>All edges used in depth-first search can be classified into two categories: <span><strong>back-edge</strong></span> or <span><strong>tree-edge</strong></span>. Back-edges point back higher up into the graph and tree-edges point lower to unexplored vertices.</p> <p>Other edges are <span><strong>forward-edges</strong></span>. These are like back-edges but they point forward instead of back into the graph.</p> <p>A <span><strong>strongly connect comonent</strong></span> is a sub-graph that contains paths between a group of vertices.</p> <h2 data-number="6.3" id="paths-and-walks"><span class="header-section-number">6.3</span> Paths and Walks</h2> <p>Paths and walks are just an extension of searching graphs.</p> <p>A <span><strong>walk</strong></span> is just a sequence of vertices that are connected. A <span><strong>path</strong></span> is a walk that contains no repeating vertices.</p> <p>To find a path between to vertices, we can use the search algorithms. The only change we make is that when we get the neighbors of a vertex, we record which vertex we got there from.</p> <p>This way, when we finally get to our end vertex, we can actually trace the vertices we travelled to arrive to the end.</p> <h2 data-number="6.4" id="minimum-spanning-trees"><span class="header-section-number">6.4</span> Minimum Spanning Trees</h2> <p>So far, everything we have talked about has been for unweighted graphs. Weighted graphs have slightly different properties and allow different operations.</p> <p>One operation is a minimum spanning tree. This is a tree containing a portion of all edges where the sum of these edges is a minimum.</p> <p>There are a few algorithms to determine the minimum spanning tree in a weighted graph but we’ll discuss Prim’s algorithm.</p> <p>Note that in an unweighted graph, there can be many MSTs. A valid MST is one that contains all the vertices with the fewest number of edges.</p> <h3 data-number="6.4.1" id="prims-algorithm"><span class="header-section-number">6.4.1</span> Prim’s Algorithm</h3> <p>Prim’s algorithm is a surprisingly simple algorithm that can be fairly quick depending on the data structure used.</p> <p>It’s not very difficult to understand either.</p> <p>First, we pick a starting node and add it to a list of new vertices. Now we will find the cheapest edge from our starting node. This edge will be added to our list of new edges and the vertex that it connects to will be added to our list of new vertices.</p> <p>Now we search for the cheapest edge that connects to either vertex in our list of new vertices. And we add the new vertex and its edge to our lists.</p> <p>Now repeat the last paragraph until there are no more edges/vertices left. This will construct a minimum spanning tree.</p> <p>As for performance, it depends on the data structure used. This most expensive part is finding the minimum edge that starts from a vertex in our list of new vertices.</p> <p>If a priority queue was used to keep the edges from the list of new vertices, we could very quick obtain the minimum edge.</p> <p>There are more ways to increase performance. The description of Dijkstra’s algorithm below is very close to a quick implementation of Prim’s algorithm.</p> <h3 data-number="6.4.2" id="kruskals-algorithm"><span class="header-section-number">6.4.2</span> Kruskal’s Algorithm</h3> <p>Although no details will be included, it’s worth mentioning another algorithm for MSTs.</p> <p>Kurstal’s algorithm is another algorithm that can find MSTs. Kruskal’s algorithm finds the MST by using a priority queue of the cheapest edges. It treats all vertices as if they’re their own connected components.</p> <p>It takes the cheapest edge, checks if it’s part of two separate components, and if it is, then the two vertices in the edge get merged into a single connected component.</p> <p>This is done until the priority queue of edges is empty.</p> <p>The performance of Kruskal’s algorithm depends on how long it takes to figure out if two vertices are part of the same connected component.</p> <p>There is a data structure called the <span><strong>union-find</strong></span> data structure that can help reduce the time it takes to check if two vertices are part of the same component.</p> <h2 data-number="6.5" id="shortest-path"><span class="header-section-number">6.5</span> Shortest Path</h2> <p>A very common problem with graphs is finding the shortest path between two vertices. This can be found everywhere in real life too. A great example is finding the quickest route between two points on a map.</p> <h3 data-number="6.5.1" id="dijkstras-algorithm"><span class="header-section-number">6.5.1</span> Dijkstra’s Algorithm</h3> <p>Dijkstra’s algorithm is an algorithm that helps find the shortest path between two vertices.</p> <p>The way it works is by finding the cost to visit all vertices from a starting vertex. But it does this in a way that only the minimum cost is ever considered.</p> <p>An implementation is also fairly simple. First, we pick a starting vertex. Now we will maintain a priority queue that we’ll be throwing a bunch of vertices in.</p> <p>After we’ve initialized, we grab all the neighbors of our starting point and throw them in the priority queue. The values in the priority queue are based on the cost to get to that neighbour from the starting point. This cost could be based on any comparison. An example might be geometric distance. We also keep track of each node’s total cost from start and which node we took to get to the neighbour which in this case was the starting point.</p> <p>Now we repeat but grab the neighbour with the lowest cost from the priority queue. Next, we add its neighbours to the priority queue. We repeat this process until we’ve found the destination node or our priority queue is empty, i.e. destination node is not in connected component.</p> <p>To actually construct the path, we just need to follow the vertex that we arrived at the destination node from all the way to the starting point.</p> <h4 data-number="6.5.1.1" id="complexity-2"><span class="header-section-number">6.5.1.1</span> Complexity</h4> <p>The performance of dijkstra’s algorithm depends largely on finding the neighbour with the minimum cost. A priority queue helps keep this cost to a minimum.</p> <p>Another way to improve performance is only finding the min of the neighbours you encounter. This is the way it worked in the description above. An alternative approach is to find the min among all vertices every iteration. That is slow and unnecessary.</p> <p>Note that Dijkstra’s algorithm is very similar to Prim’s algorithm. Infact, small changes to the above implementation would turn it into Prim’s algorithm.</p> <h2 data-number="6.6" id="toplogical-sort"><span class="header-section-number">6.6</span> Toplogical Sort</h2> <p>This is a way to sort data that is in the form of graphs.</p> <p>Easiest algorithm is to get all nodes with no incoming edges. Then, add all these nodes to the end of a sorted array. Remove all outgoing edges from these nodes. Start from finding nodes with no incoming edges again.</p> <p>These are used in any sort of dependency graph.</p> <p>Practically, build a list of nodes with no incoming edges and pop off that list. With the popped node, check the adjacency lists to remove its dependency. Also, add to list of nodes with no incoming edges while removing the dependencies.</p> <h1 data-number="7" id="searching"><span class="header-section-number">7</span> Searching</h1> <h2 data-number="7.1" id="binary-search"><span class="header-section-number">7.1</span> Binary Search</h2> <p>Binary search is one of the most amazing algorithms in computer science. It is the iconic divide-and-conquer algorithm.</p> <p>It’s also quite simple to understand. Given a sorted list, we can identify a range where our item might be. At first the range extends to the whole array. But if we check the middle of the array, we can rule out half of the entire range.</p> <p>For example, if the middle item is <code>5</code> and we’re looking for <code>2</code> then we know that the useful range is the first half. The latter half cannot contain <code>2</code> because it is sorted.</p> <p>Now we do this process recursively, always dividing our range into halves until we cannot anymore. At this point we either find our item or the item is not in the array.</p> <h2 data-number="7.2" id="linear-search"><span class="header-section-number">7.2</span> Linear Search</h2> <p>Linear search might be the simplest algorithm in computer science. We just check every item in a set if it is the item we are searching for.</p> <h2 data-number="7.3" id="backtracking"><span class="header-section-number">7.3</span> Backtracking</h2> <p>In a lot of problems, data won’t be sorted and there will be a lot of data. Sometimes, you may only have implicit data. In cases like these, linear search and binary search are out of the question.</p> <p>But we can use something called <span><strong>combinatorial</strong></span> <span><strong>searching</strong></span> to help find the solution. These are similar to brute-force but a little more clever. They should still only be used when no other solution exists. Backtracking is a form of combinatorial search.</p> <p>The idea of backtracking is fairly simple. Instead of computing all the possible solutions to a problem and then using the optimal solution, we can build up the solution one step at a time eliminating lots of possible dead-ends along the way.</p> <p>Backtracking is very similar to depth-first search. Infact, backtracking works on implicit solution graphs.</p> <p>To use backtracking, the solution must be possible to find in a number of steps. These steps can be of any type. An example might be finding all permutations of a string. This is usually done in steps: picking the first letter, then another, etc.</p> <h3 data-number="7.3.1" id="visualizing"><span class="header-section-number">7.3.1</span> Visualizing</h3> <p>The general backtracking algorithm starts with an empty solution. It will then create a list of all possible steps onwards from the current state.</p> <p>The function that creates all the new possible steps holds a lot of power. It can implement many pruning methods and fill in the new moves in a smart way.</p> <p><span><strong>Pruning</strong></span> is a technique used in backtracking to quickly rule out some solutions saving time. If you find that after adding a step, the solution is definitely wrong then we can prune that sub-tree and move on saving valuable time.</p> <p>It will then add the new step to our solution and check if it is the solution we are looking for. If it’s not that it’ll continue adding steps until it cannot anymore.</p> <p>When it’s stuck and can’t add any more steps, it will start to remove steps until it can add another new step. Then it will go down that path like before.</p> <h3 data-number="7.3.2" id="sudoku"><span class="header-section-number">7.3.2</span> Sudoku</h3> <p>The sudoku example in The Algorithm Design Manual may be the best example for illustrating backtracking and efficient pruning.</p> <p>The gist of it is as follows. To solve a sudoku puzzle, we will fill in solutions one by one until every item is filled.</p> <p>But the hard part is figuring out which space to fill and what possible values can go in that space. The example illustrates that even simple pruning drastically reduces running time.</p> <p>Smart pruning, by looking ahead in a solution for possible values, dramatically reduces the algorithm to pretty much instantaneous.</p> <p>The takeaway is that even if there is no perfect algorithm to solve a problem, we can usually do much better than simple brute force.</p> <h2 data-number="7.4" id="simulated-annealing"><span class="header-section-number">7.4</span> Simulated Annealing</h2> <p>Combinatorial search finds the optimal solution faster than brute-force but it is still going to be very slow on extremely large data sets.</p> <p>For these situations, we can use <span><strong>heuristic</strong></span> <span><strong>searching</strong></span>. We have no guarantee of finding the optimal solution but we can try to get as close as possible. Simulated annealing is a form of heurstic search.</p> <p>The basic idea is as following. If we think of solutions as a graph (calculus), then there are local minimums and maximums. But ideally, we want the global maximum, i.e. the best solution.</p> <p>One way to solve this is with a local search. This searches nearby the initial solution for a better solution. The problem with this approach is getting stuck in local maximums or minimums.</p> <p>To solve this, use simulated annealing. Simulated annealing borrows ideas from actual annealing in metallurgy. When the temperature of metal is high, atoms generally lose energy causing temperature to drop. But the temperature is the average energy. Some atoms can jump to a higher energy as well. But the probablity of an atom to jump to a higher energy level is lower at lower temperatures.</p> <p>Simulated annealing works in a similar way. We choose an initial solution and temperature. Now we start looping while looking for other solutions randomly nearby.</p> <p>If this solution is better than one current solution, we definitely move to it. If it’s worse then it’s tricky. We might move to a worse solution if the temperature is high enough and the solution isn’t too much worse.</p> <p>And every <span class="math inline">\\(n\\)</span> iterations, we decrease the temperature. The temperature is decreased exponentially.</p> <p>What happens is that while the temperature is high, the algorithm will be trying out all sorts of solutions even if they’re a little worse hoping to find the global maximum elsewhere. As the temperature slows, it’ll become more conservative with taking risks on worse solutions.</p> <p>The algorithm quits when we’ve found a satifactory solution or after a certain number of iterations.</p> <p>This heuristic is very good at solving difficult problems without algorithms. It can be used on the travelling salesman problem to get a solution very close to the best special-purpose algorithms for that problem.</p> <p>The actual implementation is complicated and not mentioned here. Just remember that this option is available for problems.</p> <h2 data-number="7.5" id="random-sampling"><span class="header-section-number">7.5</span> Random Sampling</h2> <p>Random sampling is another form of heuristic searching. This is best for cases where there doesn’t appear to be a way to logically come to a solution.</p> <p>The best bet in those cases is just to randomly select a sample and check if it can be a solution.</p> <h1 data-number="8" id="dynamic-programming"><span class="header-section-number">8</span> Dynamic Programming</h1> <p>At it’s core, dynamic programming is about improving an algorithm by storing partial results to avoid recomputing.</p> <h2 data-number="8.1" id="memoization"><span class="header-section-number">8.1</span> Memoization</h2> <p>Memoization is a key dynamic programming concept. It’s best to use to the canonical example to illustrate memoization.</p> <p>If we were to try and calculate the fibonacci sequence for some number n.</p> <p><span class="math display">\\[fib(n) = fib(n-1) + fib(n-2)\\]</span> Note how the fibonacci sequence is recursive. This means that there’s overlap between the recursions. To see this, look at the flow in calculating <span class="math inline">\\(fib(6)\\)</span>.</p> <p><span class="math display">\\[fib(6) = fib(5) + fib(4)\\]</span><span class="math display">\\[fib(5) = fib(4) + fib(3)\\]</span><span class="math display">\\[@fib(4) = fib(3) + fib(2)\\]</span> Note how we’ve already had to compute <span class="math inline">\\(fib(4)\\)</span> twice? Once when calculating <span class="math inline">\\(fib(6)\\)</span> and once in calculating <span class="math inline">\\(fib(5)\\)</span>.</p> <p>Memoization solves this problem by caching the results of computations that we may use later. In this case, we can store every fibonacci value we calculate.</p> <p>This allows us to check if we’ve already calculated a fibonacci number and then retrieve the value in constant time if possible. This approach completely removes any repeated computations.</p> <p>The impact of memoization of fibonacci numbers is enormous. We bring the complexity from <span class="math inline">\\(O(2^n)\\)</span> down to <span class="math inline">\\(O(n)\\)</span>.</p> <p>This was possible from the tradeoff of repeated computation with using more space. This is the downside of memoization.</p> <p>Note that in this specific example, if we were really clever, we’d notice that the fibonacci sequence only ever needs to store the last two fibonacci numbers. If we do exactly that, we can still calculate the fibonacci sequence in linear time while using minimal space.</p> <h2 data-number="8.2" id="longest-common-subsequence"><span class="header-section-number">8.2</span> Longest Common Subsequence</h2> <p>The problem of finding the longest common subsequence is another great example of memoizing and dynamic programming dramatically reducing the complexity.</p> <p>First, the algorithm. I’ve had more trouble with this algorithm than most others. The explanation may be a bit verbose.</p> <p>Let’s say we have two strings, A and B. Remember how strings are recursive structures? We can use that property now. Let’s say we decide to compare the last character in the strings.</p> <p>There are only two cases: they match or they don’t match. If they match, then we know that whatever the LCS (longest common subsequence) is, it must contain this character. So now let’s just cut this character off both A and B and find the LCS of the smaller strings. We know that the overall length of the LCS must be <code>1 + LCS(A[:-1] + B[:-1])</code>.</p> <p>What about the other case? If they don’t match, then there are two possibilities. We know that both <code>A[-1]</code> and <code>B[-1]</code> can’t be in the LCS. But that still leaves the case where neither or one of them is in the LCS. For those cases, we can call <code>max(LCS(A[:-1] + B), LCS(A + B[:-1]))</code></p> <p>This would take care of the three cases when the characters do not match. Now we just need an end condition. That’s simple. When either string has a length of zero, we know we cannot go any further and the LCS of the strings must be zero.</p> <p>This algorithm will give the length of the longest subsequence. To visualize it, let’s look at a simple example.</p> <h3 data-number="8.2.1" id="example"><span class="header-section-number">8.2.1</span> Example</h3> <p>Let’s find the LCS of <code>Boo</code> and <code>Look</code>.</p> <p>We check the last characters, <code>o</code> and <code>k</code> are not the same so now we must call LCS on the shorter strings <code>Bo, Look</code> and <code>Boo, Loo</code>.</p> <p>For <code>Bo, Look</code>, the last character does not match again so we call LCS for <code>B, Look</code> and <code>Bo, Loo</code>.</p> <p>On the other side, we have <code>Boo, Loo</code> which we use to find <code>Bo, Loo</code> and <code>Boo, Lo</code>.</p> <p>This is getting tedious but note that we’ve already have to find <code>Bo, Loo</code> in two different places.</p> <p>This already shows an opportunity for dynamic programming. If we could keep a dictionary that contains the LCS values and pass it between calls, we could save a lot of time. We can do exactly this by storing LCS values using the length of the two strings.</p> <p>If we were to not memoize anything, the algorithm would be exponential but if we do memoize partial results, we can get it down to just <span class="math inline">\\(O(mn)\\)</span> where <span class="math inline">\\(m = length(A)\\)</span> and <span class="math inline">\\(n = length(B)\\)</span>.</p> <h3 data-number="8.2.2" id="strings"><span class="header-section-number">8.2.2</span> Strings</h3> <p>The algorithm we talked about above will return the length of the longest common subsequence but it won’t actually tell us what that subsequence is.</p> <p>We can build another function that uses the memo filled by <code>lcs</code> to return the string.</p> <p>The idea is simple, picture the memo storing LCS values in a dict using the lengths of two strings. This memo can be visualized as a 2D table. The rows and columns are made up of letters from the two strings.</p> <p>Our goal is to start off at the bottom right and end up at the top left. The path we take is decided by the following rules:</p> <p>If the last character in the strings match, we print that character and call the function recursively on the strings minus that last character.</p> <p>If the last characters do not match, we compare the values above and to the left of our current value in our 2D table. We follow whichever side has the greater LCS value.</p> <p>Note that our current value is just the lengths of our two strings, i.e. <code>memo[(len(s1), len(s2))]</code>.</p> <p>Eventually, you will get to an empty string and return.</p> <h3 data-number="8.2.3" id="longest-increasing-subsequence"><span class="header-section-number">8.2.3</span> Longest Increasing Subsequence</h3> <p>A naive method to find the longest increasing subsequence is to sort a copy of the sequence. Then find the longest common subsequence between the raw sequence and the sorted sequence. This runs in <span class="math inline">\\(O(n^2)\\)</span>.</p> '}}]);