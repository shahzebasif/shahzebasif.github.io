"use strict";(self.webpackChunkshahzebasif=self.webpackChunkshahzebasif||[]).push([[867],{1867:(e,a,s)=>{s.r(a),s.d(a,{default:()=>n});const n='<nav id="TOC"> <ul> <li><a href="#references" id="toc-references"><span class="toc-section-number">1</span> References</a></li> <li><a href="#disclaimer" id="toc-disclaimer"><span class="toc-section-number">2</span> Disclaimer</a></li> <li><a href="#linear-regression" id="toc-linear-regression"><span class="toc-section-number">3</span> Linear Regression</a></li> <li><a href="#neural-nets" id="toc-neural-nets"><span class="toc-section-number">4</span> Neural Nets</a> <ul> <li><a href="#types" id="toc-types"><span class="toc-section-number">4.1</span> Types</a></li> <li><a href="#structure" id="toc-structure"><span class="toc-section-number">4.2</span> Structure</a> <ul> <li><a href="#embeddings" id="toc-embeddings"><span class="toc-section-number">4.2.1</span> Embeddings</a></li> </ul></li> <li><a href="#effectiveness" id="toc-effectiveness"><span class="toc-section-number">4.3</span> Effectiveness</a></li> </ul></li> <li><a href="#how" id="toc-how"><span class="toc-section-number">5</span> How?</a></li> <li><a href="#implications" id="toc-implications"><span class="toc-section-number">6</span> Implications</a></li> <li><a href="#fun" id="toc-fun"><span class="toc-section-number">7</span> Fun</a></li> </ul> </nav> <h1 data-number="1" id="references"><span class="header-section-number">1</span> References</h1> <ul> <li><p><a href="https://www.coursera.org/learn/machine-learning/home/info">Coursera Supervised ML</a></p></li> <li><p><a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">What is ChatGPT Doing</a></p></li> <li><p><a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture">Google Embeddings</a></p></li> <li><p><a href="https://www.ibm.com/cloud/blog/supervised-vs-unsupervised-learning">Supervised vs. Unsupervised</a></p></li> </ul> <h1 data-number="2" id="disclaimer"><span class="header-section-number">2</span> Disclaimer</h1> <p>I actually don’t really understand any of this at anything other than a cursory level. These are notes from basically a quick dive into the pool to see if I want to swim later.</p> <h1 data-number="3" id="linear-regression"><span class="header-section-number">3</span> Linear Regression</h1> <p>Fit a linear equation to a set of data points. This can be a first order or higher equation.</p> <p>Derived by taking existing data points and fitting the equation to them. The constants on each variable are adjusted to minimize the error between the model and the actual data points.</p> <p>There’s something called a descent algorithm which, i think, is one of the ways of reducing error in steps.</p> <h1 data-number="4" id="neural-nets"><span class="header-section-number">4</span> Neural Nets</h1> <p>Neural nets seem to take the idea of linear regression and just make them literally a million or billion times bigger.</p> <h2 data-number="4.1" id="types"><span class="header-section-number">4.1</span> Types</h2> <p>Supervised learning is when you ask it to classify a bunch of things based on labelled data provided, e.g. OCR.</p> <p>Unsupervised just takes unlabelled data and tries to cluster it or do associations, e.g. LLMs.</p> <h2 data-number="4.2" id="structure"><span class="header-section-number">4.2</span> Structure</h2> <p>There’s an input layer with a lot of input variables.</p> <p>Then, there’s a bunch of nodes, i.e. neurons, in the middle called the hidden layers.</p> <p>If I understand it right, each neuron basically does the dot product of the output of all the nodes in the previous layer by the weight of all those edges.</p> <p>The same loss function idea is used here with millions of dimensions to get to an ideal state.</p> <h3 data-number="4.2.1" id="embeddings"><span class="header-section-number">4.2.1</span> Embeddings</h3> <p>There are these things called embeddings which, I think, are basically a clustering neural net to basically make it more accurate and faster. I’m not totally sure honestly but it’s something along these lines.</p> <h2 data-number="4.3" id="effectiveness"><span class="header-section-number">4.3</span> Effectiveness</h2> <p>Apparently, we don’t really have any theory on how to optimize a neural net. It’s a lot of best practices and accidental discoveries and experimentation to get to a decent result.</p> <h1 data-number="5" id="how"><span class="header-section-number">5</span> How?</h1> <p>No one really understands how these things work. They just happen to be really good approximators for human-like output.</p> <h1 data-number="6" id="implications"><span class="header-section-number">6</span> Implications</h1> <p>The most interesting part of the recent ChatGPT and Bing Chat stuff is the point Wolfram made, maybe humans are less complicated than we think we are. Perhaps, the neural net can somehow approximate our language and mannerisms because it is basically how the real version works.</p> <p>The implications of this would be massive in neuroscience and philosophy. We can’t solve the hard problem of conciousness but this is like a hint that maybe we’re not as far away from P-zombies as we think we are.</p> <h1 data-number="7" id="fun"><span class="header-section-number">7</span> Fun</h1> <ul> <li><p><a href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html">NYT Bing Chat</a></p></li> <li><p><a href="https://old.reddit.com/r/bing/comments/1143opq/sorry_you_dont_actually_know_the_pain_is_fake/">Pain is Fake</a></p></li> </ul> '}}]);